{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMAPSS Dataset DS02 - Utils\n",
    "\n",
    "\n",
    "Please run the following cell to load all the packages required in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from keras import backend as K\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Auxiliary functions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variables(data, figsize=10, option='idx', labelsize=16):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    plt.clf()\n",
    "    \n",
    "    input_dim = len(data[0]['variables'])\n",
    "    cols = min(np.floor(input_dim**0.5).astype(int),4)\n",
    "    rows = (np.ceil(input_dim / cols)).astype(int)\n",
    "    gs   = gridspec.GridSpec(rows, cols)\n",
    "    fig  = plt.figure(figsize=(figsize, max(figsize, rows*2)))       \n",
    "    \n",
    "    # Plot dataset types\n",
    "    for n in range(input_dim):\n",
    "        ax = fig.add_subplot(gs[n])\n",
    "        # Plot only units lines\n",
    "        for jj in data[0]['ds_name']:\n",
    "            ax.plot(data[0][jj]['x'], data[0][jj]['y'][:,n], markeredgewidth=0.25, markersize=8)\n",
    "                          \n",
    "        # Adjusments\n",
    "        if (y_min !=None) & (y_max !=None):\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "        if x_max !=None:    \n",
    "            ax.set_xlim(0, x_max)\n",
    "        ax.tick_params(axis='x', labelsize=labelsize)\n",
    "        ax.tick_params(axis='y', labelsize=labelsize)\n",
    "        plt.ylabel(data[0]['label'][n], fontsize=labelsize)\n",
    "        \n",
    "        # Labels and legend\n",
    "        if option=='idx':\n",
    "            plt.xlabel('Time [t]', fontsize=labelsize)\n",
    "        else:\n",
    "            plt.xlabel('Time [cycles]', fontsize=labelsize)    \n",
    "        plt.legend(data[0]['legend'], fontsize=labelsize-2, loc='lower left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def plot_predicted_true_rul(log_y_hat_test, unit_sel, Unit_test, C_test, rul_test):\n",
    "\n",
    "    for i in range(len(log_y_hat_test)):\n",
    "        fig = plt.figure(figsize=(9, 7))\n",
    "        leg = []\n",
    "        \n",
    "        # Plot predicted RUL\n",
    "        for j in unit_sel:\n",
    "            y_hat_mean, y_hat_max, y_hat_min = [], [], []\n",
    "            unit = Unit_test == j  \n",
    "            c_test = np.sort(C_test[unit])-1\n",
    "            idx = np.argsort(C_test[unit])\n",
    "            y_hat_test = log_y_hat_test[i][unit]\n",
    "            y_hat_test_sorted = y_hat_test[idx]\n",
    "            for k in np.unique(c_test):\n",
    "                y_hat_mean.append(np.mean(y_hat_test_sorted[c_test == k]))\n",
    "                y_hat_max.append(np.max(y_hat_test_sorted[c_test == k]))\n",
    "                y_hat_min.append(np.min(y_hat_test_sorted[c_test == k]))\n",
    "            y_hat_mean = np.array(y_hat_mean, dtype=np.float64)\n",
    "            y_hat_max = np.array(y_hat_max, dtype=np.float64)\n",
    "            y_hat_min = np.array(y_hat_min, dtype=np.float64)\n",
    "            plt.plot(np.unique(c_test), y_hat_mean, 'o', alpha=0.7, markersize=5)\n",
    "            plt.fill_between(np.unique(c_test), y_hat_min, y_hat_max, alpha=0.3)\n",
    "            leg.append('Unit ' + str(j))\n",
    "        \n",
    "        # Plot true RUL\n",
    "        plt.gca().set_prop_cycle(None)\n",
    "        for j in unit_sel:        \n",
    "            unit = Unit_test == j  \n",
    "            c_test_unique = np.unique(np.sort(C_test[unit])-1)\n",
    "            rul_test_unique = np.unique(rul_test[unit])\n",
    "            plt.plot(c_test_unique, rul_test_unique[::-1], alpha=0.7)           \n",
    "            leg.append('True-Unit ' + str(j))\n",
    "        plt.legend(leg, loc='upper right')\n",
    "        plt.ylabel(r'Predicted & True $RUL$ [cycles]')\n",
    "        plt.xlabel('Time [cycles]')\n",
    "        plt.ylim(top=90)\n",
    "    \n",
    "def score_cal(y_hat, Y_test):\n",
    "    d = y_hat - Y_test\n",
    "    d.ravel()\n",
    "    score = []\n",
    "    for i in range(d.shape[0]):\n",
    "        if d[i] >= 0:\n",
    "            score.append(np.exp(d[i]/10) - 1)\n",
    "        else:\n",
    "            score.append(np.exp(-d[i]/13) - 1)\n",
    "    return np.array(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gneven/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = '../data/CMAPSS_Dataset_DS02_Assigment.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a6e9a1fa7ef8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m           'HPT_eff_mod', 'LPT_eff_mod', 'LPT_flow_mod']\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_IN\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mSOURCE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhdf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m# Nominal Training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mW_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'W_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;31m# W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '../data/CMAPSS_Dataset_DS02_Assigment.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "TEST = '../data'\n",
    "PATH_IN = TEST\n",
    "SOURCE = 'CMAPSS_Dataset_DS02_Assigment'\n",
    "\n",
    "#def load_data(PATH_IN, SOURCE)\n",
    " \n",
    "# Time tracking, Operation time (min):  0.004\n",
    "t = time.clock()\n",
    "\n",
    "# Variable name\n",
    "varname = ['alt', 'Mach', 'TRA', 'T2',\n",
    "          'T24', 'T30', 'T40', 'T48', 'T50', \n",
    "          'P15', 'P2', 'P21', 'P24', 'Ps30', 'P30', 'P40', 'P50',\n",
    "          'Nf', 'Nc', 'Wf',\n",
    "          'HPT_eff_mod', 'LPT_eff_mod', 'LPT_flow_mod']\n",
    "\n",
    "with h5py.File(PATH_IN + \"/\" + SOURCE + '.h5', 'r') as hdf:\n",
    "# Nominal Training set\n",
    "    W_train = np.array(hdf.get('W_train'))                 # W\n",
    "    X_s_train = np.array(hdf.get('X_s_train'))             # X_s\n",
    "    T_train = np.array(hdf.get('T_train'))                 # T\n",
    "    Y_train = np.array(hdf.get('Y_train'))                 # RUL  \n",
    "    U_train = np.array(hdf.get('U_train'))                 # Units\n",
    "    C_train = np.array(hdf.get('C_train'))                 # Cycles\n",
    "\n",
    "# Nominal Test set - Past\n",
    "    W_test = np.array(hdf.get('W_test'))                   # W\n",
    "    X_s_test = np.array(hdf.get('X_s_test'))               # X_s\n",
    "    T_test = np.array(hdf.get('T_test'))                   # T\n",
    "    Y_test = np.array(hdf.get('Y_test'))                   # RUL  \n",
    "    U_test = np.array(hdf.get('U_test'))                   # Units\n",
    "    C_test = np.array(hdf.get('C_test'))                   # Cycles\n",
    "\n",
    "# Nominal Test set - Future\n",
    "    W_path1 = np.array(hdf.get('W_path1'))                 # W\n",
    "    X_s_path1 = np.array(hdf.get('X_s_path1'))             # X_s\n",
    "    T_path1 = np.array(hdf.get('T_path1'))                 # T\n",
    "    Y_path1 = np.array(hdf.get('Y_path1'))                 # RUL  \n",
    "    U_path1 = np.array(hdf.get('U_path1'))                 # Units\n",
    "    C_path1 = np.array(hdf.get('C_path1'))                 # Cycles\n",
    "\n",
    "# Alternative paths\n",
    "    T_path2 = np.array(hdf.get('T_path2'))                 # T\n",
    "    T_path3 = np.array(hdf.get('T_path3'))                 # T\n",
    "\n",
    "# Noisy Test set\n",
    "    X_s_test_db60 = np.array(hdf.get('X_s_test_db60'))     # X_s\n",
    "\n",
    "# De-Noised Training set\n",
    "    X_s_train_deno = np.array(hdf.get('X_s_train_deno'))   # X_s\n",
    "\n",
    "# De-Noised Test set\n",
    "    X_s_test_deno = np.array(hdf.get('X_s_test_deno'))     # X_s \n",
    "\n",
    "print('')\n",
    "print(\"Operation time (min): \" , (time.clock()-t)/60)\n",
    "print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Units train set: \", np.unique(U_train))\n",
    "print (\"Units test set: \", np.unique(U_test))\n",
    "print('')\n",
    "print (\"W_train shape: \", W_train.shape)\n",
    "print (\"W_test shape: \", W_test.shape)\n",
    "print('')\n",
    "print (\"X_s_train shape: \", X_s_train.shape)\n",
    "print (\"X_s_test shape: \", X_s_test.shape)\n",
    "print('')\n",
    "print (\"T_train shape: \", T_train.shape)\n",
    "print (\"T_test shape: \", T_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up\n",
    "data, data[0] = {}, {}\n",
    "data[0]['variables'] = varname[-3:]       # Variables names\n",
    "print('')\n",
    "print(data[0]['variables'])\n",
    "\n",
    "data[0]['label'] = data[0]['variables']\n",
    "data[0]['legend'] = ['$true$']\n",
    "data[0]['ds_name'] = ['true']\n",
    "data[0]['option'] = 'Save'\n",
    "\n",
    "# Data to plot\n",
    "data[0]['true'], data[0]['pred'] = {}, {}\n",
    "data[0]['true']['y'] = np.concatenate((T_train, T_test, T_path1))\n",
    "data[0]['true']['x'] = np.arange(data[0]['true']['y'].shape[0])\n",
    "\n",
    "# Plot limits\n",
    "x_max = None              # Max x\n",
    "y_min = None              # Min y\n",
    "y_max = None              # Max y\n",
    "\n",
    "# Plot\n",
    "plot_variables(data, figsize=12, option='idx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Pure Data-Driven Prognostics Model with Supervised Learning (SL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up\n",
    "MODEL_PATH = '../model'\n",
    "model_sel = MODEL_PATH + '/' + 'model_FF_0'\n",
    "with open(model_sel + '.json', \"r\") as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "rul_model = model_from_json(loaded_model_json)\n",
    "rul_model.load_weights(model_sel + '.h5')\n",
    "print('')\n",
    "print(\"Loaded RUL model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)  [15 points] Prognostics based on Condition Monitoring Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict RUL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat_test = rul_model.predict(x=np.concatenate((W_test, X_s_test), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate RUL performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = score_cal(Y_hat_test, Y_test)\n",
    "print('')\n",
    "print(r's:', np.sum(scores))\n",
    "print('RMSE:', np.sqrt(np.mean((Y_hat_test - Y_test)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot predicted Vs. true RUL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True and predict vs time - Test Per engine\n",
    "unit_sel = [11, 14, 15]\n",
    "log_y_hat_test = [Y_hat_test]\n",
    "\n",
    "# Plot rul\n",
    "plot_predicted_true_rul(log_y_hat_test, unit_sel, U_test, C_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Prognostics with Generated Sensor Readings $X_{s*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)  [10 points] Generation GT Sensor Readings Given Future Trajectory.\n",
    "\n",
    "To start off the model, we will initialize `x0` with curetn sensor readings.\n",
    "\n",
    "Here are some of the key steps you'll need to implement inside the for-loop that generates the $T_y$ output characters: \n",
    "\n",
    "Step 2.A: Select `x` value according to `out` and expand `x` with `w` and `theta` i.e. `[w, out, theta]` \n",
    "\n",
    "Step 2.B: Perform one step of MLP network to get the output for the current step. \n",
    "\n",
    "Step 2.C: Save the output you have just generated by appending it to `outputs`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt_generative_model(model_sel, W, T, x0, n_w, n_t, n_out):\n",
    "    \"\"\"\n",
    "    Uses the trained inference model to generate a sequence of values.\n",
    "    \n",
    "    Arguments:\n",
    "    W -- np.array, operative conditions \n",
    "    T -- np.array, degradation condition\n",
    "    x0 -- np.array, inital sensor readings\n",
    "    n_w -- integer, dimensions of the operative conditions vector, w^{(t)}\n",
    "    n_t -- integer, dimensions of the calibtation factors vector, \\theta^{(t)}\n",
    "    n_out -- integer, number of units in the ouput layer\n",
    "    \n",
    "    Returns:\n",
    "    outputs -- np.array with predict gt_model response (i.e, $x_s$) \n",
    "    \"\"\"\n",
    "    \n",
    "    # Set-up\n",
    "    Ty = W.shape[0]\n",
    "    outputs, inputs = [], [] # Create an empty list of \"outputs\" to later store your predicted values\n",
    "\n",
    "    # Step 0: Define x0, initial input of the MLP\n",
    "    out = x0\n",
    "    \n",
    "    # Step 1: Lad inference_model\n",
    "\n",
    "        \n",
    "    # Step 2: Loop over Ty and generate a value at every time step\n",
    "    for t in range(Ty):\n",
    "        \n",
    "        # Step 2.A: Select \"x\" value according to \"out\" and expand \"x\" with \"w\" and \"theta \"i.e. \"[w, out, theta]\"\n",
    "        \n",
    "        # Step 2.B: Perform one step of MLP network\n",
    "        \n",
    "        # Step 2.C: Append the prediction \"out\" to \"outputs\". out.shape = (None, n_out)\n",
    "        \n",
    "    return outputs, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_given_input(model_sel, W_gen, T_gen, x_0, n_w, n_t, n_out):\n",
    "    \"\"\"\n",
    "    Predicts the next value of values using the generative model.\n",
    "    \n",
    "    Arguments:\n",
    "    x_initializer -- numpy array of shape (1, n_x), input vector initializing the values generation\n",
    "    W_gen -- numpy array of shape (Ty, n_w), input matrix with the w values for generation\n",
    "    T_gen -- numpy array of shape (Ty, n_t), input matrix with the \\theta values for generation\n",
    "    \n",
    "    Returns:\n",
    "    results -- numpy-array of shape (Ty, 1, n_y), matrix representing the x_s values generated\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Use the generative model to predict an output sequence given: W_gen, theta_gen, x_initializer.\n",
    "    pred, x = gt_generative_model(model_sel, W_gen, T_gen, x_0, n_w, n_t, n_out)\n",
    "    \n",
    "    # Step 2: Convert \"pred\" into an np.array()\n",
    "    results = np.array(pred)\n",
    "    inputs = np.array(x)\n",
    "    \n",
    "    return results[:,0, :], inputs[:,0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Future Conditions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nominal future trajectories\n",
    "\n",
    "# Set-up\n",
    "n_out = X_s_path1.shape[-1]\n",
    "n_w = W_path1.shape[-1]\n",
    "n_t = T_path1.shape[-1]\n",
    "\n",
    "# X0\n",
    "x_gen = X_s_path1\n",
    "x_0 = x_gen[1,:]\n",
    "x_0.shape = (1, n_out)\n",
    "print('X0 shape: ', x_0.shape)\n",
    "\n",
    "# w\n",
    "W_gen = W_path1\n",
    "print('W_gen shape: ', W_gen.shape)\n",
    "\n",
    "C_gen  = C_path1\n",
    "U_gen  = U_path1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_path2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time tracking, Operation time (min):  0.4 per prediction\n",
    "t = time.clock()\n",
    "\n",
    "model_sel = MODEL_PATH + '/' + 'model' + '_h_0_run_0'\n",
    "\n",
    "X_s_hat_inf_1, X_inf_1 = predict_given_input(model_sel, W_gen, T_path1, x_0, n_w, n_t, n_out)\n",
    "X_s_hat_inf_2, X_inf_2 = predict_given_input(model_sel, W_gen, T_path2, x_0, n_w, n_t, n_out)\n",
    "X_s_hat_inf_3, X_inf_3 = predict_given_input(model_sel, W_gen, T_path3, x_0, n_w, n_t, n_out)\n",
    "\n",
    "# Report time\n",
    "print(\"Operation time (min): \" , (time.clock()-t)/60)\n",
    "print('')\n",
    "print (\"X_s_hat_inf shape: \" + str(X_s_hat_inf_1.shape))\n",
    "print (\"X_inf shape: \" + str(X_inf_1.shape))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit 11\n",
    "mask_11 = np.ravel((U_gen == 11))\n",
    "\n",
    "# Unit 14\n",
    "mask_14 = np.ravel((U_gen == 14))\n",
    "\n",
    "# Unit 15\n",
    "mask_15 = np.ravel((U_gen == 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up\n",
    "data, data[0] = {}, {}\n",
    "data[0]['variables'] = varname[-3:]       # Variables names\n",
    "print('')\n",
    "print(data[0]['variables'])\n",
    "\n",
    "data[0]['label'] = data[0]['variables']\n",
    "data[0]['legend'] = ['$path_1$', '$path_2$', '$path_3$']\n",
    "data[0]['ds_name'] = ['path_1', 'path_2', 'path_3']\n",
    "data[0]['option'] = 'Save'\n",
    "\n",
    "# Data to plot\n",
    "data[0]['path_1'] = {}\n",
    "data[0]['path_1']['y'] = T_path1[mask_14]\n",
    "data[0]['path_1']['x'] = np.arange(data[0]['path_1']['y'].shape[0])\n",
    "\n",
    "data[0]['path_2'] = {}\n",
    "data[0]['path_2']['y'] = T_path2[mask_14]\n",
    "data[0]['path_2']['x'] = np.arange(data[0]['path_2']['y'].shape[0])\n",
    "\n",
    "data[0]['path_3'] = {}\n",
    "data[0]['path_3']['y'] = T_path3[mask_14]\n",
    "data[0]['path_3']['x'] = np.arange(data[0]['path_3']['y'].shape[0])\n",
    "\n",
    "# Plot limits\n",
    "x_max = None              # Max x\n",
    "y_min = None              # Min y\n",
    "y_max = None              # Max y\n",
    "\n",
    "# Plot\n",
    "plot_variables(data, figsize=12, option='idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up\n",
    "dim1, dim2 = 1000, 2000\n",
    "data, data[0] = {}, {}\n",
    "data[0]['variables'] = varname[4:20]       # Variables names\n",
    "print('')\n",
    "print(data[0]['variables'])\n",
    "\n",
    "data[0]['label'] = data[0]['variables']\n",
    "data[0]['legend'] = ['$path_1$', '$path_2$', '$path_3$']\n",
    "data[0]['ds_name'] = ['path_1', 'path_2', 'path_3']\n",
    "data[0]['option'] = 'Save'\n",
    "\n",
    "# Data to plot\n",
    "data[0]['path_1'] = {}\n",
    "data[0]['path_1']['y'] = X_s_hat_inf_1[dim1:dim2,:] - X_s_hat_inf_1[dim1:dim2,:]\n",
    "data[0]['path_1']['x'] = np.arange(data[0]['path_1']['y'].shape[0])\n",
    "\n",
    "data[0]['path_2'] = {}\n",
    "data[0]['path_2']['y'] = X_s_hat_inf_1[dim1:dim2,:] - X_s_hat_inf_2[dim1:dim2,:]\n",
    "data[0]['path_2']['x'] = np.arange(data[0]['path_2']['y'].shape[0])\n",
    "\n",
    "data[0]['path_3'] = {}\n",
    "data[0]['path_3']['y'] = X_s_hat_inf_1[dim1:dim2,:] - X_s_hat_inf_3[dim1:dim2,:]\n",
    "data[0]['path_3']['x'] = np.arange(data[0]['path_3']['y'].shape[0])\n",
    "\n",
    "# Plot limits\n",
    "x_max = None              # Max x\n",
    "y_min = None              # Min y\n",
    "y_max = None              # Max y\n",
    "\n",
    "# Plot\n",
    "plot_variables(data, figsize=12, option='idx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  (b)  [10 points] Prognostics with Generated Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up\n",
    "MODEL_PATH = 'C:/Users/arimanue/polybox/teaching/2020/model'\n",
    "model_sel = MODEL_PATH + '/' + 'model_FF_0'\n",
    "with open(model_sel + '.json', \"r\") as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "rul_model = model_from_json(loaded_model_json)\n",
    "rul_model.load_weights(model_sel + '.h5')\n",
    "print('')\n",
    "print(\"Loaded RUL model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict RUL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rul_path_1 = rul_model.predict(x=np.concatenate((W_gen, X_s_hat_inf_1), axis=-1))\n",
    "rul_path_2 = rul_model.predict(x=np.concatenate((W_gen, X_s_hat_inf_2), axis=-1))\n",
    "rul_path_3 = rul_model.predict(x=np.concatenate((W_gen, X_s_hat_inf_3), axis=-1))\n",
    "\n",
    "log_rul_path_gen  = [rul_path_1, rul_path_2, rul_path_3] "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "EG0F7",
   "launcher_item_id": "cxJXc"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
